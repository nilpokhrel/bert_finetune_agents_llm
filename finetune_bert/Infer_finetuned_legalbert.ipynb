{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Inference and test of legal Bert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZErwplpYhWG",
        "outputId": "060d0247-361b-41a5-a2f9-655f227bd4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install transformers pandas datasets torch scikit-learn\n",
        "!pip install safetensors\n",
        "# !pip install --upgrade transformers\n",
        "!pip install evaluate\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZsiXcubj0Gm",
        "outputId": "73732893-25a4-41d6-e52a-83bf7a643e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Collecting safetensors\n",
            "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: safetensors, transformers\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.4.5\n",
            "    Uninstalling safetensors-0.4.5:\n",
            "      Successfully uninstalled safetensors-0.4.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "Successfully installed safetensors-0.5.2 transformers-4.48.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade safetensors transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUtIW4-_as4o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from safetensors import SafetensorError  # Import SafetensorError\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UyWpMQfaz3k"
      },
      "outputs": [],
      "source": [
        "model_version_path = '/content/sample_data/model_files'\n",
        "TEXT_CLASSES = ['Approval','Fill Form','Notify'] # Filter Test Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtmM-YS26d5f"
      },
      "outputs": [],
      "source": [
        "# format prompt, query and extra supporting documents\n",
        "def primary_process_agent_format_prompt(agent_prompt, query, supporting_documents):\n",
        "  n_words = len(query.split())\n",
        "  if n_words < 4:\n",
        "    agent_prompt = agent_prompt.format(supporting_documents,'few','words')\n",
        "  else:\n",
        "    agent_prompt = agent_prompt.format(supporting_documents,'at least five','words')\n",
        "  return agent_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9gEu7JZas10"
      },
      "outputs": [],
      "source": [
        "# Collect and merge multiple csv,excel files into single dataframe\n",
        "def combine_multiple_files(files_dir_path):\n",
        "  combined_files = pd.DataFrame()\n",
        "  for files in os.listdir(files_dir_path):\n",
        "    if files.endswith('.tsv'):\n",
        "      f_path = os.path.join(files_dir_path,files)\n",
        "      df = pd.read_csv(f_path,sep='\\t')\n",
        "      combined_files = pd.concat([combined_files,df])\n",
        "    elif files.endswith('.csv'):\n",
        "      f_path = os.path.join(files_dir_path,files)\n",
        "      df = pd.read_csv(f_path,sep=',')\n",
        "      combined_files = pd.concat([combined_files,df])\n",
        "    elif files.endswith('.xlsx'):\n",
        "      f_path = os.path.join(files_dir_path,files)\n",
        "      df = pd.read_excel(f_path)\n",
        "      combined_files = pd.concat([combined_files,df])\n",
        "  return combined_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-03ljoiaspt"
      },
      "outputs": [],
      "source": [
        "def merge_multiple_dataframe(dataframe_list,merge_by=0):\n",
        "  df = pd.concat(dataframe_list, ignore_index=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQhCaaxdasm5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_varicon_pattern(text):\n",
        "    # Regular expression to match '(varicon <number>)' at the end of the string\n",
        "    cleaned_text = re.sub(r'\\s*\\(variation \\d+\\)\\s*$', '', text)\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeZhDCRAJuzE"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Remove leading and trailing spaces from the text\n",
        "    text = text.strip()\n",
        "\n",
        "    # Ensure only one space exists between sentences\n",
        "    text = re.sub(r'\\s*\\.\\s*', '. ', text)  # Handle spaces around periods\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces in general\n",
        "\n",
        "    # Ensure no space before the first sentence and after the last sentence\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlAXA53HkObO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def text_preprocessing(texts_data_, col_map_dict=None, replace_col_values_dict=None, text_k='Text', label_k='labels',func_apply=None):\n",
        "\n",
        "  if isinstance(texts_data_, pd.DataFrame):\n",
        "\n",
        "    texts_data = texts_data_.copy()\n",
        "    texts_data = texts_data.dropna().reset_index(drop=True)\n",
        "\n",
        "    n_data_input = texts_data.shape[0]\n",
        "    print('Number of Unprocessed datas: ',n_data_input)\n",
        "\n",
        "    dropped_null_entry_rows = texts_data.shape[0]\n",
        "    print('Null datas: ', n_data_input - dropped_null_entry_rows)\n",
        "\n",
        "    n_dup_data = texts_data.duplicated().sum()\n",
        "    print('Number of Duplicate datas: ',n_dup_data)\n",
        "\n",
        "    if texts_data.shape[0] < 1:\n",
        "      raise ValueError('No Data Found')\n",
        "\n",
        "    if n_dup_data > 0:\n",
        "      texts_data = texts_data.drop_duplicates().reset_index(drop=True)\n",
        "      print('After Removing Duplicates: ',texts_data.shape[0])\n",
        "\n",
        "    if col_map_dict is not None:\n",
        "      texts_data.rename(columns=col_map_dict, inplace=True)\n",
        "\n",
        "\n",
        "    # Check if the label column exists after renaming\n",
        "    if label_k not in texts_data.columns:\n",
        "      raise KeyError(f\"Column '{label_k}' not found in DataFrame after renaming. Available columns: {texts_data.columns.tolist()}\")\n",
        "\n",
        "    if replace_col_values_dict is not None:\n",
        "      texts_data[label_k] = texts_data[label_k].replace(replace_col_values_dict, regex=True)\n",
        "\n",
        "    texts_data = texts_data[[text_k, label_k]]\n",
        "    texts_data = texts_data.query(f'{label_k} in @TEXT_CLASSES')\n",
        "\n",
        "    if func_apply is not None:\n",
        "      texts_data[text_k] = texts_data[text_k].apply(func_apply)\n",
        "\n",
        "    texts_data[text_k] = texts_data[text_k].apply(clean_text)\n",
        "\n",
        "    texts_data.drop_duplicates(inplace=True)\n",
        "    texts_data.reset_index(inplace=True, drop=True)\n",
        "    print(texts_data.head(3))\n",
        "    value_counts = texts_data[label_k].value_counts()\n",
        "    print(\"\\nFrequency of each unique value in Task Type: \")\n",
        "    print(value_counts)\n",
        "    return texts_data\n",
        "\n",
        "  elif isinstance(texts_data, str):\n",
        "    # texts_data = remove_varicon_pattern(texts_data)\n",
        "    return texts_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCcWG3bFB4QM"
      },
      "outputs": [],
      "source": [
        "# Initialize Classifier Bert Model\n",
        "def initialize_legal_bert_model(_model_path):\n",
        "\n",
        "  files_ = !ls {_model_path}\n",
        "  print('Files of Model: ',files_)\n",
        "  computing_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  try:\n",
        "      model = AutoModelForSequenceClassification.from_pretrained(_model_path, local_files_only=True)\n",
        "      tokenizer = AutoTokenizer.from_pretrained(_model_path, local_files_only=True)\n",
        "  except SafetensorError:\n",
        "    print('SafetensorError,  Initializing Manually ...')\n",
        "\n",
        "    # Load weights manually if safetensor fails to load model\n",
        "    state_dict = torch.load(f\"{model_version_path}/pytorch_model.bin\", weights_only=True, map_location=torch.device(computing_device))\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(None, state_dict=state_dict)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_version_path)\n",
        "\n",
        "  except (OSError): # Catch potential file access errors or other Transformer-related errors\n",
        "    print('Error loading model. Check file paths and integrity.')\n",
        "    # If there's still an issue, raise the exception for debugging\n",
        "    raise\n",
        "\n",
        "  model = model.to(computing_device)\n",
        "  print('Model Running on Device: ',computing_device)\n",
        "  # create model classifier\n",
        "  classifier = pipeline('text-classification',model=model,tokenizer=tokenizer)\n",
        "  return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIzNaF50aseC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# we can use pipeline for Inference\n",
        "def run_custom_bert_model(text, classifier, class_mapper=None):\n",
        "\n",
        "  if class_mapper is None:\n",
        "      # Map model labels to meaningful names\n",
        "      mapping_label = {\n",
        "          'LABEL_2': 'Notify',\n",
        "          'LABEL_0': 'Fill Form',\n",
        "          'LABEL_1': 'Approval'\n",
        "      }\n",
        "  else:\n",
        "      mapping_label = class_mapper\n",
        "\n",
        "  # Get probabilities for all class labels\n",
        "  results = classifier(text, top_k=None)\n",
        "\n",
        "  # Map results to custom label names with scores\n",
        "  map_results = {mapping_label[result['label']]: result['score'] for result in results}\n",
        "  return map_results,(text, max(map_results, key=map_results.get), map_results[max(map_results, key=map_results.get)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBJVkizlivW5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMNv-y9ldlIw"
      },
      "outputs": [],
      "source": [
        "# Random text samples\n",
        "def infer_model_test_sample(texts,text_key=None,text_label=None):\n",
        "  if isinstance(texts, str):\n",
        "    texts = texts\n",
        "    _, results = run_custom_bert_model(texts, classifier, class_mapper=None)\n",
        "    return results\n",
        "\n",
        "  elif isinstance(texts,(list | tuple)):\n",
        "\n",
        "    result = []\n",
        "    for text in texts:\n",
        "      res_dict = {}\n",
        "      _, results = run_custom_bert_model(text, classifier, class_mapper=None)\n",
        "      res_dict['labels'] = results[1]\n",
        "      res_dict['prob'] = results[2]\n",
        "      res_dict['text'] = text\n",
        "      result.append(res_dict)\n",
        "    return result\n",
        "\n",
        "  elif isinstance(texts, pd.DataFrame):\n",
        "    texts = texts.dropna().reset_index(drop=True)\n",
        "    predicted_label = []\n",
        "    original_label = []\n",
        "    pred_prob = []\n",
        "    org_text = []\n",
        "\n",
        "    if text_key is None:\n",
        "      text_key = 'Text'\n",
        "    if text_label is None:\n",
        "      text_label = 'labels'\n",
        "\n",
        "    for data in  texts.iterrows():\n",
        "      # Get probabilities for all class labels\n",
        "      text = data[1][text_key]\n",
        "      label = data[1][text_label]\n",
        "      original_label.append(label)\n",
        "      org_text.append(text)\n",
        "      _, response = run_custom_bert_model(text, classifier, class_mapper=None)\n",
        "      predicted_label.append(response[1])\n",
        "      pred_prob.append(response[2])\n",
        "    df = pd.DataFrame({'Text':org_text,'Original_Label':original_label,'Predicted_Label':predicted_label,'Probability_Score':pred_prob})\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peC5wDkXZpLM"
      },
      "outputs": [],
      "source": [
        "def prediction_analysis(report_, save_report_csv=False):\n",
        "  if isinstance(report_, pd.DataFrame):\n",
        "    total_test_cases = len(report_)\n",
        "    correct_predictions = report_[report_['Original_Label'] == report_['Predicted_Label']].reset_index(drop=True)\n",
        "    correct_predictions_count = len(correct_predictions)\n",
        "    incorrect_predictions = report_[report_['Original_Label'] != report_['Predicted_Label']].reset_index(drop=True)\n",
        "    incorrect_predictions_count = len(incorrect_predictions)\n",
        "    accuracy = (correct_predictions_count / total_test_cases) * 100\n",
        "    print(f\"Total Test Cases: {total_test_cases}\")\n",
        "    print(f\"Correct Predictions: {correct_predictions_count}\")\n",
        "    print(f\"Incorrect Predictions: {incorrect_predictions_count}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    if save_report_csv:\n",
        "      correct_predictions.to_csv('correct_predicted.csv')\n",
        "      incorrect_predictions.to_csv('incorrect_predicted.csv')\n",
        "    return correct_predictions, incorrect_predictions\n",
        "  else:\n",
        "    raise ValueError(\"Invalid input type. Expected DataFrame, list[dict], or dict.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud5pXrcTBCPx"
      },
      "outputs": [],
      "source": [
        "def read_text_report(df):\n",
        "  if isinstance(df, pd.DataFrame):\n",
        "    for row in df.iterrows():\n",
        "      print('*'*20)\n",
        "      print(f\"Text: {row[1]['Text']}\")\n",
        "      print(f\"Original Label: {row[1]['Original_Label']}\")\n",
        "      print(f\"Predicted Label: {row[1]['Predicted_Label']}\")\n",
        "      print('*'*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbwsqEUYyvh5"
      },
      "outputs": [],
      "source": [
        "text_sample = (\n",
        "    'start the project scoping process by filling out the required form with all the necessary details. Also need to capture the necessary details about the project in a form, ensuring everything aligns with the intended objectives and regulatory requirements. In the next step I need to  get final signoff from Project Manager with comments on all potential concerns are addressed. Eventually, all stakholders within this process will be sent letter with final updates.',\n",
        "    'They need to get final signoff from Project Manager with remarks on all potential concerns addressed.',\n",
        "    \"Start the project scoping process by filling out the required form with all the necessary details. \",\n",
        "    \"Also need to capture the necessary details about the project in a form, ensuring everything aligns \",\n",
        "    \"with the intended objectives and regulatory requirements. In the next step I need to get final signoff \",\n",
        "    \"from the Project Manager with comments on all potential concerns being addressed. Eventually, all stakeholders \",\n",
        "    \"within this process will be sent a letter with final updates.\",\n",
        "    'Send Information',\n",
        "    'Require customer profile data',\n",
        "    'Inquiry user documents',\n",
        "    'As part of the compliance process, I will submit a draft of the project scope to the project manager for review and approval, to confirm that all critical items align with the EU AI Act requirements.',\n",
        "    \"If approved, an email is sent to all participants.\",\n",
        "    \"Manager to assess my document for further process\",\n",
        "    \"Risk Manager approves  document.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEI9xnXUkAs5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTryG2hi3PH_",
        "outputId": "91dac7b6-7c5e-4ab1-9b19-8ae70de3d523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files of Model:  ['config.json\\t   special_tokens_map.json  tokenizer.json', 'pytorch_model.bin  tokenizer_config.json    vocab.txt']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Running on Device:  cpu\n"
          ]
        }
      ],
      "source": [
        "# INITIALIZE MODEL\n",
        "\n",
        "classifier = initialize_legal_bert_model(model_version_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WY71IJnzohI",
        "outputId": "1deb6c34-5abb-4da1-b6c8-ec5379f1e751"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'labels': 'Fill Form',\n",
              "  'prob': 0.9999583959579468,\n",
              "  'text': 'start the project scoping process by filling out the required form with all the necessary details. Also need to capture the necessary details about the project in a form, ensuring everything aligns with the intended objectives and regulatory requirements. In the next step I need to  get final signoff from Project Manager with comments on all potential concerns are addressed. Eventually, all stakholders within this process will be sent letter with final updates.'},\n",
              " {'labels': 'Approval',\n",
              "  'prob': 0.9998867511749268,\n",
              "  'text': 'They need to get final signoff from Project Manager with remarks on all potential concerns addressed.'},\n",
              " {'labels': 'Fill Form',\n",
              "  'prob': 0.9999651908874512,\n",
              "  'text': 'Start the project scoping process by filling out the required form with all the necessary details. '},\n",
              " {'labels': 'Fill Form',\n",
              "  'prob': 0.9998960494995117,\n",
              "  'text': 'Also need to capture the necessary details about the project in a form, ensuring everything aligns '},\n",
              " {'labels': 'Approval',\n",
              "  'prob': 0.9999107122421265,\n",
              "  'text': 'with the intended objectives and regulatory requirements. In the next step I need to get final signoff '},\n",
              " {'labels': 'Approval',\n",
              "  'prob': 0.9783128499984741,\n",
              "  'text': 'from the Project Manager with comments on all potential concerns being addressed. Eventually, all stakeholders '},\n",
              " {'labels': 'Approval',\n",
              "  'prob': 0.9314183592796326,\n",
              "  'text': 'within this process will be sent a letter with final updates.'},\n",
              " {'labels': 'Fill Form',\n",
              "  'prob': 0.8594013452529907,\n",
              "  'text': 'Send Information'},\n",
              " {'labels': 'Fill Form',\n",
              "  'prob': 0.99888676404953,\n",
              "  'text': 'Require customer profile data'},\n",
              " {'labels': 'Fill Form',\n",
              "  'prob': 0.9852168560028076,\n",
              "  'text': 'Inquiry user documents'},\n",
              " {'labels': 'Approval',\n",
              "  'prob': 0.9999163150787354,\n",
              "  'text': 'As part of the compliance process, I will submit a draft of the project scope to the project manager for review and approval, to confirm that all critical items align with the EU AI Act requirements.'},\n",
              " {'labels': 'Notify',\n",
              "  'prob': 0.980587363243103,\n",
              "  'text': 'If approved, an email is sent to all participants.'},\n",
              " {'labels': 'Approval',\n",
              "  'prob': 0.9578482508659363,\n",
              "  'text': 'Manager to assess my document for further process'},\n",
              " {'labels': 'Approval',\n",
              "  'prob': 0.7763791084289551,\n",
              "  'text': 'Risk Manager approves  document.'}]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "infer_model_test_sample(text_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEnSHPPO_BQu"
      },
      "source": [
        "## TEST CASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISqLmgz8-CQU",
        "outputId": "e58848d8-47b1-45f4-a296-c759904bed3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('need peer review information informed to all the coworkers',\n",
              " 'Approval',\n",
              " 0.9682497382164001)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = 'need peer review information informed to all the coworkers'\n",
        "infer_model_test_sample(query)\n",
        "\n",
        "'''\n",
        "('need peer review information informed to all the coworkers',\n",
        " 'Approval',\n",
        " 0.9682497382164001)\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
